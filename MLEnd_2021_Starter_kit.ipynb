{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLEnd_2021_Starter_kit.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvrRvdTwxnvV"
      },
      "source": [
        "# Environment set up\n",
        "\n",
        "In this section we will set up a Colab environment for the MLEnd mini-project. Before starting, follow these simple instructions: \n",
        "\n",
        "1.   Go to https://drive.google.com/\n",
        "2.   Create a folder named 'Data' in 'MyDrive': On the left, click 'New' > 'Folder', enter the name 'Data', and click 'create'\n",
        "3.   Open the 'Data' folder and create a folder named 'MLEndHW'.\n",
        "\n",
        "Let's start by loading a few useful Python libraries and mounting our personal Google Drive storage system (i.e. making it available)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gjrwd7Cxez5"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os, sys, re, pickle, glob\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import IPython.display as ipd\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfJ0Fm_ey_2f"
      },
      "source": [
        "# Data download\n",
        "\n",
        "In this section we will download a small subsample of the MLEnd Hums and Whistles Dataset. \n",
        "\n",
        "You should be able to download the entire training dataset using a similar approach to the one used here for the subsample. As you will see, you only need to provide a different link. \n",
        "\n",
        "Please note that even though we call it \"training\" dataset you can do whatever you want with it (for instance validate a set of models). We have kept a separate dataset for testing purposes, which we won't share with anyone.\n",
        "\n",
        "First, let's define a function that will allow us to download a file into a chosen location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPYQeCyEzKpK"
      },
      "source": [
        "def download_url(url, save_path):\n",
        "    with urllib.request.urlopen(url) as dl_file:\n",
        "        with open(save_path, 'wb') as out_file:\n",
        "            out_file.write(dl_file.read())"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnZtCTVGzPuc"
      },
      "source": [
        "The next step is to download the file 'MLEndHW_Sample.zip' into the folder 'MyDrive/Data/MLEndHW'. Note that this might take a while!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pRFkQinzRtK"
      },
      "source": [
        "url  = \"https://collect.qmul.ac.uk/down?t=4HT0J1TOL9MG9IP8/612TNHDECF8A8QC6JRSPKCG\"\n",
        "save_path = '/content/drive/MyDrive/Data/MLEndHW/MLEndHW_Sample.zip'\n",
        "download_url(url, save_path)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fddX9WV_0oWx"
      },
      "source": [
        "Run the following cell to check that the MLEndHW folder contains the file 'MLEndHW_Sample.zip':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mroJNE2m0nI9"
      },
      "source": [
        "path = '/content/drive/MyDrive/Data/MLEndHW'\n",
        "os.listdir(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD2rP-8AzWS4"
      },
      "source": [
        "# Understanding our dataset sample\n",
        "\n",
        "Let's unzip the sample data and check how many audio files we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOPTdwA9zeyz"
      },
      "source": [
        "directory_to_extract_to = '/content/drive/MyDrive/Data/MLEndHW/sample/'\n",
        "zip_path = '/content/drive/MyDrive/Data/MLEndHW/MLEndHW_Sample.zip'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)\n",
        "\n",
        "sample_path = '/content/drive/MyDrive/Data/MLEndHW/sample/MLEndHW_Sample/*.wav'\n",
        "files = glob.glob(sample_path)\n",
        "len(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0OeDvt7zedb"
      },
      "source": [
        "This figure (98) corresponds to the number of **items** or **samples** in our dataset. Let's listen to some random audio files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcqXgKzgzmwG"
      },
      "source": [
        "for _ in range(5):\n",
        "  n = np.random.randint(98)\n",
        "  display(ipd.Audio(files[n]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3qkV3Im2WET"
      },
      "source": [
        "Can you recognise each song? Can you tell whether the interpreters are humming or whistling to the song?\n",
        "\n",
        "Let's have a look at the name of the 98 audio file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHa-FqEg2Gyu"
      },
      "source": [
        "for file in files:\n",
        "  print(file.split('/')[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh3--lJb5AEk"
      },
      "source": [
        "As you can see, the name of each file follows the naming convention `[Participant ID]_[type of recording]_[interpretation number]_[song]`. We can parse each file name and extract this information. Let's do it for the first one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOg8q-yE4_Dr"
      },
      "source": [
        "print('The full path to the first audio file is: ', files[0])\n",
        "print('\\n')\n",
        "print('The name of the first audio file is: ', files[0].split('/')[-1])\n",
        "print('    The participand ID is: ', files[0].split('/')[-1].split('_')[0])\n",
        "print('    The type of interpretation is: ', files[0].split('/')[-1].split('_')[1])\n",
        "print('    The interpretation number is: ', files[0].split('/')[-1].split('_')[2])\n",
        "print('    The song is: ', files[0].split('/')[-1].split('_')[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdTayCxW7t1z"
      },
      "source": [
        "We can create a table-like structure using Python lists that collects the information that we can extract from the names of the audio files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10zSKNKQFwDc"
      },
      "source": [
        "MLENDHW_table = [] \n",
        "\n",
        "for file in files:\n",
        "  file_name = file.split('/')[-1]\n",
        "  participant_ID = file.split('/')[-1].split('_')[0]\n",
        "  interpretation_type = file.split('/')[-1].split('_')[1]\n",
        "  interpretation_number = file.split('/')[-1].split('_')[2]\n",
        "  song = file.split('/')[-1].split('_')[3].split('.')[0]\n",
        "  MLENDHW_table.append([file_name,participant_ID,interpretation_type,interpretation_number, song])\n",
        "\n",
        "MLENDHW_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9Okox2iI87b"
      },
      "source": [
        "We can load the table into a Pandas DataFrame and usie the additional functionalities of Pandas to explore the information extracted from the names of the audio files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nhyvDeZGnGr"
      },
      "source": [
        "MLENDHW_df = pd.DataFrame(MLENDHW_table,columns=['file_id','participant','interpretation','number','song']).set_index('file_id') \n",
        "MLENDHW_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJVADYvuHq3x"
      },
      "source": [
        "# Feature extraction : Picth\n",
        "\n",
        "Audio files are complex data types. Specifically they are **discrete signals** or **time series**, consisting of values on a 1D temporal grid. These values are known as *samples* themselves, which might be a bit confusing, as we have used this term to refer to the *items* in our dataset. The **sampling frequency** is the rate at which samples in an audio file are produced. For instance a sampling frequency of 5HZ indicates that 5 produce 5 samples per second, or 1 sample every 0.2 s.\n",
        "\n",
        "Let's plot one of our audio signals:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU7cSdySHx8P"
      },
      "source": [
        "n=0\n",
        "fs = None # Sampling frequency. If None, fs would be 22050\n",
        "x, fs = librosa.load(files[n],sr=fs)\n",
        "t = np.arange(len(x))/fs\n",
        "plt.plot(t,x)\n",
        "plt.xlabel('time (sec)')\n",
        "plt.ylabel('amplitude')\n",
        "plt.show()\n",
        "display(ipd.Audio(files[n]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVndEmbJIR5-"
      },
      "source": [
        "Can you recognise the song and interpretation type? Does it agree with the values shown in the ``` MLENDHW_df ``` dataframe? Let's check it:\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4cSPJxnT80j"
      },
      "source": [
        "MLENDHW_df.loc[files[n].split('/')[-1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Weh65CY7T9da"
      },
      "source": [
        "Note that we are using the name of the audio file as the index in the Pandas DataFrame. In this example the actual name of the audio file happens to supply the same information, but in general this won't be the case. By changing the value of `n` in the previous cell, you can listen to other examples. If you are doing this during one of our lab sessions, please make sure that your mic is muted!\n",
        "\n",
        "Exactly, how complex is an audio signal? Let's start by looking at the number of samples in one of our audio files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oSPjIW9JZWv"
      },
      "source": [
        "n=0\n",
        "x, fs = librosa.load(files[n],sr=fs)\n",
        "print('This audio signal has', len(x), 'samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrNs96siJggo"
      },
      "source": [
        "If we are using a raw audio signal as the input of a machine learning model, we will be operating in a predictor space consisting of hundreds of thousands of dimensions. Compare this figure with the number of items (i.e. recordings) that we have. Do we have enough samples to train a model that takes one of these audio signals as an input?\n",
        "\n",
        "One approach to deal with this huge dimensionality is to extract a few features from our signals and use these features as predictors instead. In this notebook we will use four audio features, namely:\n",
        "\n",
        "\n",
        "1.   Power.\n",
        "2.   Pitch mean.\n",
        "3.   Pitch standard deviation.\n",
        "4.   Fraction of voiced region.\n",
        "\n",
        "In the next cell, we define a new function that gets the pitch of an audio signal (do not worry if you do not know what it is, but feel free to read about it!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiXYO1tdJgIs"
      },
      "source": [
        "def getPitch(x,fs,winLen=0.02):\n",
        "  #winLen = 0.02 \n",
        "  p = winLen*fs\n",
        "  frame_length = int(2**int(p-1).bit_length())\n",
        "  hop_length = frame_length//2\n",
        "  f0, voiced_flag, voiced_probs = librosa.pyin(y=x, fmin=80, fmax=450, sr=fs,\n",
        "                                                 frame_length=frame_length,hop_length=hop_length)\n",
        "  return f0,voiced_flag"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCRRi_NFM3kw"
      },
      "source": [
        "Let's consider the problem of identifying the type of interpretation of one of the audio recordings. Then next cell defines a function that takes a number of files and creates a NumPy array containing the 4 audio features used as predictors (`X`) and a binary label (`y`), that indicates whether the type of interpretation is a hum (`y=1`) or whistle (`y=0`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEMwPrVmMHZa"
      },
      "source": [
        "def getXy(files,labels_file, scale_audio=False, onlySingleDigit=False):\n",
        "  X,y =[],[]\n",
        "  for file in tqdm(files):\n",
        "    fileID = file.split('/')[-1]\n",
        "    file_name = file.split('/')[-1]\n",
        "    #print(file_name)\n",
        "    #print(labels_file.loc[fileID]['interpretation'])\n",
        "    #print(labels_file.loc[fileID]['interpretation']=='hum')\n",
        "    #yi = list(labels_file.loc[fileID]['interpretation'])[0]=='hum'\n",
        "    yi = labels_file.loc[fileID]['interpretation']=='hum'\n",
        "\n",
        "    fs = None # if None, fs would be 22050\n",
        "    x, fs = librosa.load(file,sr=fs)\n",
        "    if scale_audio: x = x/np.max(np.abs(x))\n",
        "    f0, voiced_flag = getPitch(x,fs,winLen=0.02)\n",
        "      \n",
        "    power = np.sum(x**2)/len(x)\n",
        "    pitch_mean = np.nanmean(f0) if np.mean(np.isnan(f0))<1 else 0\n",
        "    pitch_std  = np.nanstd(f0) if np.mean(np.isnan(f0))<1 else 0\n",
        "    voiced_fr = np.mean(voiced_flag)\n",
        "\n",
        "    xi = [power,pitch_mean,pitch_std,voiced_fr]\n",
        "    X.append(xi)\n",
        "    y.append(yi)\n",
        "\n",
        "  return np.array(X),np.array(y)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQdqy2ksNCD7"
      },
      "source": [
        "Let's apply `getXy` to the subsample and obtain the NumPy predictor array (`X`) and a binary label (`y`). This could take a while, as we are processing each of the 98 recordings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkjX_Zu9NNUs"
      },
      "source": [
        "X,y = getXy(files, labels_file=MLENDHW_df, scale_audio=True, onlySingleDigit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOA190Z0kSVq"
      },
      "source": [
        "The next cell shows the shape of `X` and `y` and prints the labels vector `y`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcDtFySakTeP"
      },
      "source": [
        "print('The shape of X is', X.shape) \n",
        "print('The shape of y is', y.shape)\n",
        "print('The labels vector is', y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVvbOCDBZbQx"
      },
      "source": [
        "As you can see, we have 98 items consisting of 4 features (stored in `X`) and one binary label (stored in `y`). Is our dataset balanced? Let's have a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW_grfnuYmjD"
      },
      "source": [
        "print(' The number of hum recordings is ', np.count_nonzero(y))\n",
        "print(' The number of whistle recordings is ', y.size - np.count_nonzero(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp1Kgp5OkdC-"
      },
      "source": [
        "# Modeling: Support Vector Machines\n",
        "\n",
        "Let's build a support vector machine (SVM) model for the predictive task of identifying the type of interpretation (hum/whistle) of an audio signal, using the dataset that we have just created.\n",
        "\n",
        "We will use the SVM method provided by scikit-learn and will split the dataset defined by X and y into a training set and a validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv0rBpWFkoe6"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.3)\n",
        "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqPeEFEqk_cS"
      },
      "source": [
        "Can you identify the number of items in the training and validation sets?\n",
        "\n",
        "Let's now fit an SVM model and print both the training accuracty and validation accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZYHghqhlDeM"
      },
      "source": [
        "model  = svm.SVC(C=1)\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "yt_p = model.predict(X_train)\n",
        "yv_p = model.predict(X_val)\n",
        "\n",
        "print('Training Accuracy', np.mean(yt_p==y_train))\n",
        "print('Validation  Accuracy', np.mean(yv_p==y_val))\n",
        "print('The support vectors are', model.support_vectors_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFr71A-blKsN"
      },
      "source": [
        "Compare the training and validation accuracies. Is our model overfitting, underfitting, performing well? What do you think the accuracy of a random classifier would be?\n",
        "\n",
        "Let's normalise the predictors, to see if the performance improves.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJGHrtdWlLre"
      },
      "source": [
        "mean = X_train.mean(0)\n",
        "sd =  X_train.std(0)\n",
        "\n",
        "X_train = (X_train-mean)/sd\n",
        "X_val  = (X_val-mean)/sd\n",
        "\n",
        "model  = svm.SVC(C=1,gamma=2)\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "yt_p = model.predict(X_train)\n",
        "yv_p = model.predict(X_val)\n",
        "\n",
        "print('Training Accuracy', np.mean(yt_p==y_train))\n",
        "print('Validation  Accuracy', np.mean(yv_p==y_val))\n",
        "print('The support vectors are', model.support_vectors_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMHpZlB5lPr_"
      },
      "source": [
        "What machine learning pipeline have we implemented for each solution? Feel free to try other machine learning models available in scikit, it's extremely easy!"
      ]
    }
  ]
}